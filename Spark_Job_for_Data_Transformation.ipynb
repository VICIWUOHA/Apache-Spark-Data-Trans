{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_Job_for_Data_Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "T0Tw5priumcr",
        "TfbHw_pnCcUI",
        "-cAHuZ5DGDMl",
        "57D_DzoF4T-s",
        "FlyZ0yHdqAOb",
        "WiF9wvTCBR-8",
        "BLDGPDmEg6Mw",
        "WTBoQ-0OO26W",
        "u-__FvSN9Z5g"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VICIWUOHA/Apache-Spark-Data-Trans/blob/main/Spark_Job_for_Data_Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SPARK Module for Data Cleaning**"
      ],
      "metadata": {
        "id": "T0Tw5priumcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Script built on Apache Spark was used to wrange data from a Google Cloud Data Store before writing into a Postgres Database."
      ],
      "metadata": {
        "id": "DEtCaNoLBVNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoIZ5YjJsXTj"
      },
      "outputs": [],
      "source": [
        "# Install jdk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# Unzip the downloaded file\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pyspark\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install pyarrow\n"
      ],
      "metadata": {
        "id": "xqDBaUVZvxFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5247016a-b6d4-4c6b-a992-c1c9f1f72fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 48.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=2256f07ba5aff5f5ef9e15edd229d2680a3c90c9be89f94b81b66f942ce17392\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Environment path\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n"
      ],
      "metadata": {
        "id": "PWaBibJLwCR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pyspark and initialize it within the environment\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "UrG95Kga1UQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "        \n",
        "# print to preview the spark variable and confirm that it is working\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "90t29pxR4hNo",
        "outputId": "d9d4d97f-77ef-4fde-9514-4dbe80472b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f0859dbe610>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://31a4453f7b4c:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To allow us view the Spark UI\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT6MLzjA4_8u",
        "outputId": "a041681f-208f-4d3a-b477-4f25ce77fb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-16 13:11:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  13.8MB/s    in 1.0s    \n",
            "\n",
            "2022-05-16 13:11:32 (13.8 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "{\"tunnels\":[{\"name\":\"command_line\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://6627-35-204-111-138.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}},{\"name\":\"command_line (http)\",\"uri\":\"/api/tunnels/command_line%20%28http%29\",\"public_url\":\"http://6627-35-204-111-138.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data folders into spark dataframe\n",
        "from  pyspark.sql.functions import input_file_name,substring_index,substring,regexp_replace,expr\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType,DateType\n",
        "\n",
        "# Defining Schema for the table, because the inferSchema= True arg can be faulty\n",
        "\n",
        "ordersSchema = StructType([\n",
        "# Add all fields\n",
        "StructField('Order No', StringType(), nullable=False),\n",
        "StructField('Order Time', StringType(), nullable=False),\n",
        "StructField('Order Type', StringType(), True),\n",
        "StructField('Order Taken By', StringType(), True),\n",
        "StructField('Customer Name', StringType(), True),\n",
        "StructField('Customer Number', StringType(), True),\n",
        "StructField('Item name', StringType(), True),\n",
        "StructField('Quantity', FloatType(), True),\n",
        "StructField('Item Type', StringType(), True),\n",
        "StructField('Price', FloatType(), True),\n",
        "StructField('Item Tax', FloatType(), True),\n",
        "StructField('Item discount', FloatType(), True)\n",
        "])\n",
        "Jan_Feb_product_orders = spark.read.csv('/content/drive/MyDrive/SD Raw Data/Jan-Feb 2022/Product_Orders',\n",
        "                                      header=True,\n",
        "                                      schema=ordersSchema).withColumn(\"Filename\", input_file_name())\n",
        "Jan_Feb_product_orders.printSchema()\n",
        "Jan_Feb_product_orders.show(5)\n",
        "\n",
        "April_product_orders = spark.read.csv('/content/drive/MyDrive/SD Raw Data/April 2022/Product_orders',\n",
        "                                      header=True,\n",
        "                                      schema=ordersSchema\n",
        "                                      ).withColumn(\"Filename\", input_file_name())\n",
        "# previously used method\n",
        "# April_product_orders = spark.read.csv('/content/drive/MyDrive/SD Raw Data/April 2022/Product_orders',\n",
        "#                                       header=True,\n",
        "#                                       inferSchema=True\n",
        "#                                       ).withColumn(\"Store\", input_file_name())\n",
        "April_product_orders.printSchema()\n",
        "April_product_orders.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPZ7cH17-Olb",
        "outputId": "4a943e7e-6820-4099-e634-595732b6b08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Order No: string (nullable = true)\n",
            " |-- Order Time: string (nullable = true)\n",
            " |-- Order Type: string (nullable = true)\n",
            " |-- Order Taken By: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Customer Number: string (nullable = true)\n",
            " |-- Item name: string (nullable = true)\n",
            " |-- Quantity: float (nullable = true)\n",
            " |-- Item Type: string (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- Item Tax: float (nullable = true)\n",
            " |-- Item discount: float (nullable = true)\n",
            " |-- Filename: string (nullable = false)\n",
            "\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "|Order No|          Order Time|Order Type|Order Taken By|Customer Name|Customer Number|           Item name|Quantity|   Item Type| Price|Item Tax|Item discount|            Filename|\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "|  281456|01-Jan-2022 06:21 AM|   Walk-In| Emmanuel Igba|         null|           null|Sausage & Cheese ...|     1.0|Regular Item| 900.0|     0.0|         null|file:///content/d...|\n",
            "|  281456|01-Jan-2022 06:21 AM|   Walk-In| Emmanuel Igba|         null|           null|      Americano 16oz|     1.0|Regular Item|1400.0|     0.0|         null|file:///content/d...|\n",
            "|  281457|01-Jan-2022 06:25 AM|   Walk-In| Emmanuel Igba|         null|           null|   Hot Chocolate 8oz|     2.0|Regular Item|1100.0|     0.0|         null|file:///content/d...|\n",
            "|  281457|01-Jan-2022 06:25 AM|   Walk-In| Emmanuel Igba|         null|           null| Farm Fresh Milk 8oz|     2.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|\n",
            "|  281457|01-Jan-2022 06:25 AM|   Walk-In| Emmanuel Igba|         null|           null| Tray Carrier 2 Cups|     1.0|Regular Item|   0.0|     0.0|         null|file:///content/d...|\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- Order No: string (nullable = true)\n",
            " |-- Order Time: string (nullable = true)\n",
            " |-- Order Type: string (nullable = true)\n",
            " |-- Order Taken By: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Customer Number: string (nullable = true)\n",
            " |-- Item name: string (nullable = true)\n",
            " |-- Quantity: float (nullable = true)\n",
            " |-- Item Type: string (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- Item Tax: float (nullable = true)\n",
            " |-- Item discount: float (nullable = true)\n",
            " |-- Filename: string (nullable = false)\n",
            "\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "|Order No|          Order Time|Order Type|Order Taken By|Customer Name|Customer Number|           Item name|Quantity|   Item Type| Price|Item Tax|Item discount|            Filename|\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "|  353758|01-Apr-2022 06:10 AM|   Walk-In| Emmanuel Igba|         null|           null|  Hot Chocolate 12oz|     1.0|Regular Item|1400.0|     0.0|         null|file:///content/d...|\n",
            "|  353758|01-Apr-2022 06:10 AM|   Walk-In| Emmanuel Igba|         null|           null|Dano Full Cream M...|     1.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|\n",
            "|  353758|01-Apr-2022 06:10 AM|   Walk-In| Emmanuel Igba|         null|           null|          Latte 12oz|     1.0|Regular Item|1400.0|     0.0|         null|file:///content/d...|\n",
            "|  353758|01-Apr-2022 06:10 AM|   Walk-In| Emmanuel Igba|         null|           null|Dano Full Cream M...|     1.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|\n",
            "|  353758|01-Apr-2022 06:10 AM|   Walk-In| Emmanuel Igba|         null|           null|      Dozen Assorted|     1.0|Regular Item|5700.0|     0.0|         null|file:///content/d...|\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qkZzdbHUDvnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Exploration**\n",
        "\n",
        "This is to confirm that spark read all the csv files in my folders at once.\n",
        "\n",
        "- I'll do  a Tail Preview and then Find the Length of the Dataframe.\n",
        "\n",
        "- I'll also check the Number of Distinct Orders in each dataframe, to confirm that it's what i have on my application/ Aggregated Orders Table."
      ],
      "metadata": {
        "id": "TfbHw_pnCcUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previewing top 10 and bottom 10 rows in both Jan-Feb and April Data Folders\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pyspark.sql.functions import desc,asc\n",
        "\n",
        "Jan_Feb_tail = Jan_Feb_product_orders.withColumn(\"index\", monotonically_increasing_id())\n",
        "Jan_Feb_tail.orderBy(desc(\"index\")).show(5)\n",
        "# Jan_Feb_tail = Jan_Feb_product_orders.orderBy(\"\")\n",
        "print('Jan - Feb No of Rows in df :',Jan_Feb_product_orders.count())\n",
        "\n",
        "Apr_tail = April_product_orders.withColumn(\"index\", monotonically_increasing_id())\n",
        "Apr_tail.orderBy(desc(\"index\")).drop(\"index\").show(5) #Here i dropped the index generated, above i left it there.\n",
        "print('April No of Rows in df :',April_product_orders.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I1YKsqTCtWK",
        "outputId": "ca638a89-be44-411e-cab2-c1c6a447fe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+----------+--------------+-------------+---------------+-------------------+--------+------------+------+--------+-------------+--------------------+------+\n",
            "|Order No|          Order Time|Order Type|Order Taken By|Customer Name|Customer Number|          Item name|Quantity|   Item Type| Price|Item Tax|Item discount|            Filename| index|\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+-------------------+--------+------------+------+--------+-------------+--------------------+------+\n",
            "|  329385|28-Feb-2022 06:22 PM|   Walk-In|   Abel Affang|         null|           null|  Nutty  Chocolatta|     1.0|    Modifier| 650.0|     0.0|         null|file:///content/d...|204379|\n",
            "|  329385|28-Feb-2022 06:22 PM|   Walk-In|   Abel Affang|         null|           null|           Assorted|     1.0|Regular Item|   0.0|     0.0|         null|file:///content/d...|204378|\n",
            "|  329345|28-Feb-2022 05:25 PM|   Walk-In|   Abel Affang|         null|           null|Delivery Fee (1000)|     1.0|Regular Item|1000.0|     0.0|         null|file:///content/d...|204377|\n",
            "|  329345|28-Feb-2022 05:25 PM|   Walk-In|   Abel Affang|         null|           null|   Triple Chocolate|     1.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|204376|\n",
            "|  329345|28-Feb-2022 05:25 PM|   Walk-In|   Abel Affang|         null|           null|    Red Velvet Cake|     1.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|204375|\n",
            "+--------+--------------------+----------+--------------+-------------+---------------+-------------------+--------+------------+------+--------+-------------+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Jan - Feb No of Rows in df : 204380\n",
            "+--------+--------------------+----------+-----------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "|Order No|          Order Time|Order Type|   Order Taken By|Customer Name|Customer Number|           Item name|Quantity|   Item Type| Price|Item Tax|Item discount|            Filename|\n",
            "+--------+--------------------+----------+-----------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "|  378036|30-Apr-2022 08:34 PM|   Walk-In|Olafimihan Philip|         null|           null|     Original Glazed|     3.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|\n",
            "|  378036|30-Apr-2022 08:34 PM|   Walk-In|Olafimihan Philip|         null|           null|          Joy Box OG|     1.0|Regular Item|1300.0|     0.0|         null|file:///content/d...|\n",
            "|  377921|30-Apr-2022 07:20 PM|   Walk-In|Olafimihan Philip|         null|           null|Dano Full Cream M...|     1.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|\n",
            "|  377921|30-Apr-2022 07:20 PM|   Walk-In|Olafimihan Philip|         null|           null|          Latte 12oz|     1.0|Regular Item|1400.0|     0.0|         null|file:///content/d...|\n",
            "|  377921|30-Apr-2022 07:20 PM|   Walk-In|Olafimihan Philip|         null|           null|Hershey's Cookies...|     1.0|    Modifier|   0.0|     0.0|         null|file:///content/d...|\n",
            "+--------+--------------------+----------+-----------------+-------------+---------------+--------------------+--------+------------+------+--------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "April No of Rows in df : 105251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Distinct Order Count Using **SparkSQL**"
      ],
      "metadata": {
        "id": "-cAHuZ5DGDMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N/B Because this project is propietary, some columns have been omitted "
      ],
      "metadata": {
        "id": "f6o5yCKE3pz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SQL Temp Views in SQL for both dataframes\n",
        "# After Cleaninig, when i call this, the createorReplaceTempView on the file, it will be replaced.\n",
        "April_product_orders.createOrReplaceTempView(\"AprilpdtOrders\")\n",
        "Jan_Feb_product_orders.createOrReplaceTempView(\"JanFebpdtOrders\")\n"
      ],
      "metadata": {
        "id": "unS4vog0W8RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing Order counts, used the backticks `column name` to avoid errors when querying\n",
        "# Note that the output of sparksql queries is still a spark dataframe.\n",
        "\n",
        "Jan_Feb_distinct_order_count = spark.sql(\"select count(distinct(`Order No`)) As No_of_Jan_Feb_orders from JanFebpdtOrders\")\n",
        "April_distinct_order_count = spark.sql(\"select count(distinct(`Order No`)) As No_of_Apr_orders from AprilpdtOrders\")\n",
        "\n",
        "Jan_Feb_distinct_order_count.show()\n",
        "April_distinct_order_count.show()\n",
        "\n",
        "# Showing two diff methods of printing df output of spark\n",
        "\n",
        "print('No of Orders from Jan - Feb :',Jan_Feb_distinct_order_count.head(1)[0]['No_of_Jan_Feb_orders'])\n",
        "print('No of Orders in April :', April_distinct_order_count.collect()[0]['No_of_Apr_orders'])\n",
        "\n",
        "\n",
        "# The Above satisfies my data validation that all rows of my data have been pulled in from Google Cloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCMO8SURf0yA",
        "outputId": "37510046-20ee-452a-8cba-a8c4844012c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|No_of_Jan_Feb_orders|\n",
            "+--------------------+\n",
            "|               48022|\n",
            "+--------------------+\n",
            "\n",
            "+----------------+\n",
            "|No_of_Apr_orders|\n",
            "+----------------+\n",
            "|           24307|\n",
            "+----------------+\n",
            "\n",
            "No of Orders from Jan - Feb : 48022\n",
            "No of Orders in April : 24307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning of Columns and Creation of Additional Columns.**\n",
        "\n",
        "On this data set there is need to do the following;\n",
        "\n",
        "1) Replace all the **whitespaces** in column names with underscore symbols - **_**.\n",
        "\n",
        "2) Make all column names to be in  **Camel_Case** .\n",
        "\n",
        "3) More Transformation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "57D_DzoF4T-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K2XVUNoDXflP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since Both DataFrames are in the same format, we can now merge them into a singular spark df and perform all cleaning at once.\n",
        "\n",
        "Jan_Apr_product_orders = Jan_Feb_product_orders.union(April_product_orders)\n",
        "Jan_Apr_product_orders.drop('Order No','Item name','Order Taken By').show(1)\n",
        "\n",
        "# Confirm the length of the new df\n",
        "Jan_Apr_product_orders.count()\n",
        "\n",
        "# Merge Successful\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11clz-07Jcta",
        "outputId": "d50941b0-cb01-4acd-9c00-68102acfab37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-------------+---------------+--------+------------+-----+--------+-------------+--------------------+\n",
            "|          Order Time|Order Type|Customer Name|Customer Number|Quantity|   Item Type|Price|Item Tax|Item discount|            Filename|\n",
            "+--------------------+----------+-------------+---------------+--------+------------+-----+--------+-------------+--------------------+\n",
            "|01-Jan-2022 06:21 AM|   Walk-In|         null|           null|     1.0|Regular Item|900.0|     0.0|         null|file:///content/d...|\n",
            "+--------------------+----------+-------------+---------------+--------+------------+-----+--------+-------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "309631"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Column Names"
      ],
      "metadata": {
        "id": "Ylky11VWgeYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.column import Column\n",
        "#  Clean Column names\n",
        "from pyspark.sql.functions import initcap\n",
        "from functools import reduce\n",
        "\n",
        "# Get a list of columns as python list\n",
        "current_column_names = Jan_Apr_product_orders.columns \n",
        "print(current_column_names)\n",
        "\n",
        "# Get a list of columns as python list\n",
        "new_column_names = list(map(lambda column_name : column_name.replace(\" \",\"_\"),current_column_names))\n",
        "print(new_column_names)\n",
        "\n",
        "# Map new column names to dataframe\n",
        "# Rename columns: Item_name,Order_Time,Order_No,Item_Type to [Product,Order_Date_Time,Order_Id,Product_Type] respectively\n",
        "\n",
        "Jan_Apr_product_orders = reduce(lambda data, idx: data.withColumnRenamed(current_column_names[idx], new_column_names[idx]),\n",
        "                                range(len(current_column_names)), Jan_Apr_product_orders\n",
        "                                ).withColumnRenamed('Item_name','Product'\n",
        "                                ).withColumnRenamed('Order_Time','Order_Date_Time'\n",
        "                                ).withColumnRenamed('Order_No','Order_Id'\n",
        "                                ).withColumnRenamed('Item_Type','Product_Type')\n",
        "\n",
        "Jan_Apr_product_orders.printSchema()\n",
        "# Jan_Apr_product_orders.show(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltqTSOBzScUi",
        "outputId": "a33d59c7-3a6a-44ba-b6b8-b0eac939b41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Order No', 'Order Time', 'Order Type', 'Order Taken By', 'Customer Name', 'Customer Number', 'Item name', 'Quantity', 'Item Type', 'Price', 'Item Tax', 'Item discount', 'Filename']\n",
            "['Order_No', 'Order_Time', 'Order_Type', 'Order_Taken_By', 'Customer_Name', 'Customer_Number', 'Item_name', 'Quantity', 'Item_Type', 'Price', 'Item_Tax', 'Item_discount', 'Filename']\n",
            "root\n",
            " |-- Order_Id: string (nullable = true)\n",
            " |-- Order_Date_Time: string (nullable = true)\n",
            " |-- Order_Type: string (nullable = true)\n",
            " |-- Order_Taken_By: string (nullable = true)\n",
            " |-- Customer_Name: string (nullable = true)\n",
            " |-- Customer_Number: string (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Quantity: float (nullable = true)\n",
            " |-- Product_Type: string (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- Item_Tax: float (nullable = true)\n",
            " |-- Item_discount: float (nullable = true)\n",
            " |-- Filename: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating new **derived** Columns and **dropping** unwanted columns"
      ],
      "metadata": {
        "id": "FlyZ0yHdqAOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, to_timestamp, from_unixtime,unix_timestamp,date_format\n",
        "from pyspark.sql import functions as F\n",
        "Jan_Apr_product_orders_2 = Jan_Apr_product_orders.withColumn(\"Store\",\n",
        "                                                   (regexp_replace(\n",
        "                                                       (substring_index(\n",
        "                                                           (substring_index(\n",
        "                                                             \"Filename\",'/',-1)),'.',1)),'%20',\" \"))\n",
        "                                                   ).withColumn(\"Order_Date\",to_date((substring_index(\"Order_Date_Time\",' ',1)),\"dd-MMM-yyy\")\n",
        "                                                   ).withColumn(\"Order_Time\",substring_index(\"Order_Date_Time\",' ',-2)\n",
        "                                                   ).withColumn(\"Order_Time_2\",date_format(to_timestamp(\"Order_Date_Time\",\"dd-MMM-yyyy hh:mm a\"),\"HH:mm:ss\")\n",
        "                                                   ).withColumn(\"Order_Date_Time_2\",to_timestamp(\"Order_Date_Time\",\"dd-MMM-yyyy hh:mm a\")\n",
        "                                                   ).withColumn(\"Sales\", (col(\"Price\")*col(\"Quantity\"))\n",
        "                                                   ).withColumn(\"Product_Has_Price\",F.when((col(\"Price\")==0.0),0).otherwise(1)\n",
        "                                                   ).drop(\"Item_Discount\",\"Filename\",\"Customer_Name\",\"Customer_Number\"\n",
        "                                                   )\n",
        "\n",
        "\n",
        "# X = April_product_orders.select('Store').show(5,truncate=False)\n",
        "# Y = April_product_orders.select(substring_index('Store',))\n",
        "# df.select(substring_index(df.s, '.', -3).alias('s')\n",
        "# .withColumn(\"Order_Time\",substring_index(\"Order_Date_Time\",' ',-2)\n",
        "\n",
        "# Jan_Apr_product_orders_2.show(1)\n",
        "Jan_Apr_product_orders_2.printSchema()\n",
        "# changing long file path names to shorter actual names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeJeGc0QqtX5",
        "outputId": "8a25940a-0f1a-48c4-d3c9-0c9f8b7a31b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Order_Id: string (nullable = true)\n",
            " |-- Order_Date_Time: string (nullable = true)\n",
            " |-- Order_Type: string (nullable = true)\n",
            " |-- Order_Taken_By: string (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Quantity: float (nullable = true)\n",
            " |-- Product_Type: string (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- Item_Tax: float (nullable = true)\n",
            " |-- Store: string (nullable = false)\n",
            " |-- Order_Date: date (nullable = true)\n",
            " |-- Order_Time: string (nullable = true)\n",
            " |-- Order_Time_2: string (nullable = true)\n",
            " |-- Order_Date_Time_2: timestamp (nullable = true)\n",
            " |-- Sales: float (nullable = true)\n",
            " |-- Product_Has_Price: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Validation Using Spark SQL**"
      ],
      "metadata": {
        "id": "StUKCC0tTjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me confirm that my Time column was parsed correctly using sparksql\n",
        "Jan_Apr_product_orders_2.createOrReplaceTempView(\"TestCleaned\")\n",
        "\n",
        "# Test_cleaned = spark.sql(\"\"\"SELECT \n",
        "#                               MAX(Order_Time) AS max_UNcleaned_order_time, \n",
        "#                               MIN(Order_Time) AS min_UNcleaned_order_time,\n",
        "#                               MAX(Order_Time_2) AS max_cleaned_order_time, \n",
        "#                               MIN(Order_Time_2) AS min_cleaned_order_time,\n",
        "#                               MAX(Order_Date_Time) AS max_UNcleaned_order_date_time,\n",
        "#                               MIN(Order_Date_Time) AS min_UNcleaned_order_date_time, \n",
        "#                               MAX(Order_Date_Time_2) AS max_cleaned_order_date_time,\n",
        "#                               MIN(Order_Date_Time_2) AS min_cleaned_order_date_time \n",
        "#                               FROM TestCleaned \"\"\")\n",
        "\n",
        "# Test_cleaned.show()\n",
        "\n",
        "\n",
        "# Test_Cleaned_2 = spark.sql(\"\"\"SELECT * FROM TestCleaned\n",
        "#                               WHERE Order_Time_2 = (\n",
        "#                                 SELECT MIN(Order_Time_2) AS min_cleaned_order_time\n",
        "#                                 FROM TestCleaned)\"\"\")\n",
        "\n",
        "# Test_Cleaned_3 = spark.sql(\"\"\"SELECT * FROM TestCleaned\n",
        "#                               WHERE Order_Time_2 = (\n",
        "#                                 SELECT Order_Time_2 FROM (\n",
        "#                                   SELECT Order_Time_2, ROW_NUMBER() OVER (ORDER BY Order_Time_2) AS Time_Record\n",
        "#                                 FROM TestCleaned) \n",
        "#                                 WHERE Time_Record = 130)\"\"\")\n",
        "# Test_Cleaned_3.show()"
      ],
      "metadata": {
        "id": "EGvy3Yx1utnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Add blank Columns to dataframe**\n",
        "These Columns are present in data GENERATED from another System,\n",
        "After this data cleaning, we combining (Union) this dataframe with data from the other system.\n",
        "\n",
        "After this, we would ;\n",
        "- drop the remaining wrong date & time columns \n",
        "- Rename the correct date & time columns to _**Order_date** & **Order_Time.**_\n",
        "\n",
        "###Lastly, \n",
        "We would Select the columns in the **order** which we want to ingest into our data warehouse"
      ],
      "metadata": {
        "id": "SS5ofBEXnpkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "Jan_Apr_product_orders_2 = Jan_Apr_product_orders_2.withColumn(\"Price_Without_Discount\",lit(None).cast(FloatType())\n",
        "                                                  ).withColumn(\"Unit_Cost\",lit(None).cast(FloatType())\n",
        "                                                  ).withColumn(\"Product_Id\",lit(None).cast(StringType())\n",
        "                                                  ).drop(\"Order_Date_Time\",\"Order_Time\"\n",
        "                                                  ).withColumnRenamed(\"Order_Time_2\",\"Order_Time\"\n",
        "                                                  ).na.fill({\"Price_Without_Discount\": 0.0, \"Unit_Cost\": 0.0}\n",
        "                                                  ).select(\"Order_Id\",\"Order_Date\",\n",
        "                                                           \"Order_Time\",\"Store\",\"Order_Type\",\"Order_Taken_By\",\n",
        "                                                           \"Product_Id\",\"Product\",\"Quantity\",\"Product_Type\",\n",
        "                                                           \"Price\",\"Sales\",\"Item_Tax\",\"Price_Without_Discount\",\n",
        "                                                           \"Unit_Cost\",\"Product_Has_Price\")\n",
        "# Price_Without_Discount, Unit_Cost,\n",
        "# \n",
        "\n",
        "# Jan_Apr_product_orders_2.show(5)\n",
        "Jan_Apr_product_orders_2.printSchema()\n",
        "Jan_Apr_product_orders_2.createOrReplaceTempView(\"finalProductOrders\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmHvgQsPTevP",
        "outputId": "45dc9472-248e-4fab-ba64-5b287930af82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Order_Id: string (nullable = true)\n",
            " |-- Order_Date: date (nullable = true)\n",
            " |-- Order_Time: string (nullable = true)\n",
            " |-- Store: string (nullable = false)\n",
            " |-- Order_Type: string (nullable = true)\n",
            " |-- Order_Taken_By: string (nullable = true)\n",
            " |-- Product_Id: string (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Quantity: float (nullable = true)\n",
            " |-- Product_Type: string (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- Sales: float (nullable = true)\n",
            " |-- Item_Tax: float (nullable = true)\n",
            " |-- Price_Without_Discount: float (nullable = false)\n",
            " |-- Unit_Cost: float (nullable = false)\n",
            " |-- Product_Has_Price: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Used for cleaning  store name formerly\n",
        "# April_product_orders.withColumn(\"Store_2\",\n",
        "#     regexp_replace(\n",
        "#         (substring_index(\n",
        "#             (substring_index('Store','/',-1)\n",
        "#             .alias('Store_2')),'.',1)),'%20',\" \")\n",
        "#             .alias('Store_2')\n",
        "#             ).show(5)\n",
        "# # April_product_orders.show(5)"
      ],
      "metadata": {
        "id": "mNxCGVI6CQy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Export To Csv File**\n",
        "\n",
        "- Prepare for deployment to PostgreSql DB"
      ],
      "metadata": {
        "id": "WiF9wvTCBR-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jan_Apr_product_orders_2.coalesce(1).write.option(\"header\",True\n",
        "#                                                   ).csv(\"/content/drive/MyDrive/SD Raw Data/product_orders_Jan_Feb_Apr_3\"\n",
        "#                                                         )\n",
        "\n"
      ],
      "metadata": {
        "id": "2rFj2a7NBRp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Total Orders Datasets**\n",
        "\n",
        "- Data from these files will be transformed for ready sinking into the Data Warehouse.\n",
        "\n",
        "- Expected Columns in DW\n",
        "\n",
        "  Total_Orders ---> Order_Id, Order_date, Order_Time, Store, Order_Taken_By, Order_Products_Price,Product_Count, Order_Extra_Items_Price, Order_Amount, Payment_Channel, Payment_Mode, Order_Type, Order_State."
      ],
      "metadata": {
        "id": "BLDGPDmEg6Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  'Order Time'\t' Order Type'\t' Location'\t' Order Taken By'\t' Ready At'\t' Picked At'\t' Picked By'\t' Delivered At'\t' Order Amount'\t' Payments'\t' Tax'\t' Tips'\t' Amount Received'\t' Received By'\t' Amount Returned'\t' Status'\t' Notes'\t' Consumption Tax + VAT (12.5%)'\t' Net Amount'\t' Con Tax + VAT (12.5%)'\t' Net Amount'\t' VAT (0%)'\t' Net Amount'\t' VAT (7.5%)'\t' Net Amount'\t' VAT + Consumption Tax (10%)'\t' Net Amount'\t' VAT + Consumption Tax (12.5%)'\t' Net Amount'\n",
        "TotalordersSchema = StructType([\n",
        "# Add all fields\n",
        "StructField('Order No', StringType(), nullable=False),\n",
        "StructField(' Order Time', StringType(), nullable=False),\n",
        "StructField(' Order Type', StringType(), True),\n",
        "StructField(' Location', StringType(), True),\n",
        "StructField(' Order Taken By', StringType(), True),\n",
        "StructField(' Ready At', StringType(), True),\n",
        "StructField(' Picked At', StringType(), True),\n",
        "StructField(' Picked By', StringType(), True),\n",
        "StructField(' Delivered At', StringType(), True),\n",
        "StructField(' Order Amount', FloatType(), True),\n",
        "StructField(' Payments', StringType(), True),\n",
        "StructField(' Tax', FloatType(), True),\n",
        "StructField(' Tips', FloatType(), True),\n",
        "StructField(' Amount Received', FloatType(), True),\n",
        "StructField(' Received By', StringType(), True),\n",
        "StructField(' Amount Returned', FloatType(), True),\n",
        "StructField(' Status', StringType(), True),\n",
        "StructField(' Notes', StringType(), True),\n",
        "StructField(' Consumption Tax + VAT (12.5%)', FloatType(), True),\n",
        "StructField(' Net Amount', FloatType(), True),\n",
        "StructField(' Con Tax + VAT (12.5%)', FloatType(), True),\n",
        "StructField(' Net Amount_2', FloatType(), True),\n",
        "StructField(' VAT (0%)', FloatType(), True),\n",
        "StructField(' Net Amount_3', FloatType(), True),\n",
        "StructField(' VAT (7.5%)', FloatType(), True),\n",
        "StructField(' Net Amount_4', FloatType(), True),\n",
        "StructField(' VAT + Consumption Tax (10%)', FloatType(), True),\n",
        "StructField(' Net Amount_5', FloatType(), True),\n",
        "StructField(' VAT + Consumption Tax (12.5%)', FloatType(), True),\n",
        "StructField(' Net Amount_6', FloatType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "# sd_total_orders_raw = spark.read.option(\"header\",\"true\").csv(\"/content/drive/MyDrive/SD Raw Data/Jan_Feb_Apr_Totals\",\n",
        "#                                      schema=TotalordersSchema,\n",
        "#                                      multiLine= True)\n",
        "\n",
        "sd_total_orders_raw = spark.read.option(\"header\",\"true\").csv(\"/content/drive/MyDrive/SD Raw Data/Jan_Feb_Apr_Totals\",\n",
        "                                     inferSchema=True,\n",
        "                                     multiLine= True\n",
        "                                     ).withColumn('Order No',expr(\"CAST(`Order No` as String)\"))\n",
        "                                \n",
        "\n",
        "sd_total_orders_raw.printSchema()\n",
        "# sd_total_orders_raw.show(5)\n",
        "print(' No of Rows in files : ', sd_total_orders_raw.count())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhbOMhCEhQC3",
        "outputId": "74030ba4-8f03-4469-e414-d9d24defed80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Order No: string (nullable = true)\n",
            " |--  Order Time: string (nullable = true)\n",
            " |--  Order Type: string (nullable = true)\n",
            " |--  Location: string (nullable = true)\n",
            " |--  Order Taken By: string (nullable = true)\n",
            " |--  Ready At: string (nullable = true)\n",
            " |--  Picked At: string (nullable = true)\n",
            " |--  Picked By: string (nullable = true)\n",
            " |--  Delivered At: string (nullable = true)\n",
            " |--  Order Amount: string (nullable = true)\n",
            " |--  Payments: string (nullable = true)\n",
            " |--  Tax: double (nullable = true)\n",
            " |--  Tips: double (nullable = true)\n",
            " |--  Amount Received: double (nullable = true)\n",
            " |--  Received By: string (nullable = true)\n",
            " |--  Amount Returned: double (nullable = true)\n",
            " |--  Status: string (nullable = true)\n",
            " |--  Notes: string (nullable = true)\n",
            " |--  Consumption Tax + VAT (12.5%): double (nullable = true)\n",
            " |--  Net Amount19: double (nullable = true)\n",
            " |--  Con Tax + VAT (12.5%): double (nullable = true)\n",
            " |--  Net Amount21: double (nullable = true)\n",
            " |--  VAT (0%): string (nullable = true)\n",
            " |--  Net Amount23: string (nullable = true)\n",
            " |--  VAT (7.5%): double (nullable = true)\n",
            " |--  Net Amount25: double (nullable = true)\n",
            " |--  VAT + Consumption Tax (10%): string (nullable = true)\n",
            " |--  Net Amount27: string (nullable = true)\n",
            " |--  VAT + Consumption Tax (12.5%): string (nullable = true)\n",
            " |--  Net Amount29: string (nullable = true)\n",
            "\n",
            " No of Rows in files :  72329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Notes on Importing File**\n",
        "\n",
        " 1) The Syntax, .withColumn(\" Tax\",expr(\"CAST(` Tax` as String)\")) - is Used here to automatically  restructure the Tax column's data type.\n",
        " 2) Again for some reason, without the multiline = True Argument, i was getting some errors on the number of rows being imported.\n",
        "More rows were being imported, because when looking at the csv on a notepad, it was evident that some data from some rows were scattered across multiple lines.\n",
        "\n",
        "Explanation of this concept can be found here https://sparkbyexamples.com/spark/spark-read-multiline-multiple-line-csv-file/\n",
        "\n"
      ],
      "metadata": {
        "id": "ZL4pL14cGEf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Column Names\n",
        "\n",
        "old names = ['']\n"
      ],
      "metadata": {
        "id": "WTBoQ-0OO26W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There is need to drop some columns which may just have null values\n",
        "\n",
        "# Let me do some descriptive overview\n",
        "# sd_total_orders_raw.describe().show()\n",
        "\n",
        "# It's evident that i have columns that are completely empty, I'll drop them in the next line.\n",
        "# But to avoid errors, let me curate all the column names\n",
        "sd_old_col_names = sd_total_orders_raw.columns\n",
        "print(sd_old_col_names)\n",
        "\n",
        "\"\"\" The next is to replace all the spaces in the column names with underscore but trim the ones before  them\n",
        "here i want apply a different logic from what i used above, first, \n",
        "i will strip all the column names above of the leading white spaces,i would use \n",
        ".strip() incase there are also trailing spaces.\n",
        "Next up i will replace all the white spaces and special characters with underscore\n",
        " \"\"\"\n",
        "import re\n",
        "sd_new_col_names = list(re.sub(r'\\W+','_',(column.strip())) for column in sd_total_orders_raw.columns)\n",
        "# list((column.strip().replace(' ', '_') for column in sd_total_orders_raw.columns))\n",
        "  \n",
        "print(sd_new_col_names)\n",
        "\n",
        "# I now have a clean list of column names to be mapped back to my dataframe\n",
        "\n",
        "sd_total_orders_withcolrenamed = sd_total_orders_raw.toDF(*sd_new_col_names)\n",
        "# sd_total_orders_withcolrenamed.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DYkz7BLIlpX",
        "outputId": "d2932771-3353-40e7-c059-e7cb8fdf2c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Order No', ' Order Time', ' Order Type', ' Location', ' Order Taken By', ' Ready At', ' Picked At', ' Picked By', ' Delivered At', ' Order Amount', ' Payments', ' Tax', ' Tips', ' Amount Received', ' Received By', ' Amount Returned', ' Status', ' Notes', ' Consumption Tax + VAT (12.5%)', ' Net Amount19', ' Con Tax + VAT (12.5%)', ' Net Amount21', ' VAT (0%)', ' Net Amount23', ' VAT (7.5%)', ' Net Amount25', ' VAT + Consumption Tax (10%)', ' Net Amount27', ' VAT + Consumption Tax (12.5%)', ' Net Amount29']\n",
            "['Order_No', 'Order_Time', 'Order_Type', 'Location', 'Order_Taken_By', 'Ready_At', 'Picked_At', 'Picked_By', 'Delivered_At', 'Order_Amount', 'Payments', 'Tax', 'Tips', 'Amount_Received', 'Received_By', 'Amount_Returned', 'Status', 'Notes', 'Consumption_Tax_VAT_12_5_', 'Net_Amount19', 'Con_Tax_VAT_12_5_', 'Net_Amount21', 'VAT_0_', 'Net_Amount23', 'VAT_7_5_', 'Net_Amount25', 'VAT_Consumption_Tax_10_', 'Net_Amount27', 'VAT_Consumption_Tax_12_5_', 'Net_Amount29']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Preview the df again and then drop unwanted columns**"
      ],
      "metadata": {
        "id": "iFoij_O70BdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I'll perform the descriptive overview again before drpopping  null columns\n",
        "\n",
        "# sd_total_orders_withcolrenamed.describe().show()\n",
        "\n",
        "non_null_columns = [column for column in sd_total_orders_withcolrenamed.columns if sd_total_orders_withcolrenamed.filter(F.col(column).isNotNull()).count() > 0]\n",
        "sd_total_orders_withdroppednull_cols = sd_total_orders_withcolrenamed.select(*non_null_columns)\n",
        "\n",
        "\n",
        "sd_total_orders_withdroppednull_cols = sd_total_orders_withdroppednull_cols.withColumnRenamed('Order_Time', 'Order_Date_Time'\n",
        "                                                                          ).withColumnRenamed('Order_No', 'Order_Id')\n",
        "# sd_total_orders_withdroppednull_cols.show(5)\n",
        "\n",
        "# At this point, we have dropped completely null columns\n",
        "\n",
        "# sd_total_orders_withcolrenamed.show(5)"
      ],
      "metadata": {
        "id": "WgmIy6S40M7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Processing Data Types for Columns**\n",
        "\n",
        "- Extract Order_Date and Order_Time from Order_Time which is a datetime column.\n",
        "- Our Order_Amount Column ia also faulty, we need to remove the commas and cast it as a float.\n",
        "- Replace null for all remaining numeric columns with 0\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgqsYOkhBw7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sd_total_orders_withdroppednull_cols = sd_total_orders_withdroppednull_cols.withColumn('Order_Date', to_date((substring_index(\"Order_Date_Time\",' ',1)),\"dd-MMM-yyy\")\n",
        "                                                                          ).withColumn('Order_Time', date_format(to_timestamp(\"Order_Date_Time\",\"dd-MMM-yyyy hh:mm a\"),\"HH:mm:ss\")\n",
        "                                                                          ).withColumn('Order_Amount',F.regexp_replace(col(\"Order_Amount\"),\",\",'').cast(FloatType())\n",
        "                                                                          ).drop('Order_Date_Time')\n",
        "\n",
        "# sd_total_orders_withdroppednull_cols.show(5)\n",
        "\n",
        "# validate schema \n",
        "\n",
        "sd_total_orders_withdroppednull_cols.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFwnLvLvCKBd",
        "outputId": "129b2fa7-6055-47ff-b1d0-703013963900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Order_Id: string (nullable = true)\n",
            " |-- Order_Type: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Order_Taken_By: string (nullable = true)\n",
            " |-- Ready_At: string (nullable = true)\n",
            " |-- Order_Amount: float (nullable = true)\n",
            " |-- Payments: string (nullable = true)\n",
            " |-- Tax: double (nullable = true)\n",
            " |-- Tips: double (nullable = true)\n",
            " |-- Amount_Received: double (nullable = true)\n",
            " |-- Received_By: string (nullable = true)\n",
            " |-- Amount_Returned: double (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            " |-- Notes: string (nullable = true)\n",
            " |-- Consumption_Tax_VAT_12_5_: double (nullable = true)\n",
            " |-- Net_Amount19: double (nullable = true)\n",
            " |-- Con_Tax_VAT_12_5_: double (nullable = true)\n",
            " |-- Net_Amount21: double (nullable = true)\n",
            " |-- VAT_7_5_: double (nullable = true)\n",
            " |-- Net_Amount25: double (nullable = true)\n",
            " |-- Order_Date: date (nullable = true)\n",
            " |-- Order_Time: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract Necessary Tables from Dataframe**"
      ],
      "metadata": {
        "id": "z5JeL00qkwB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Extract the Raw tax data from the Main Dataframe\n",
        "\n",
        "sd_total_orders_withdroppednull_cols.createOrReplaceTempView('sd_Total_Orders')\n",
        "\n",
        "# sd_Taxes = sd_total_orders_withdroppednull_cols.filter(F.col('Tax') > 0\n",
        "#                                               ).select('Order_Id','Order_Date','Consumption_Tax_VAT_12_5_','Net_Amount19','Con_Tax_VAT_12_5_','Net_Amount21','VAT_7_5_','Net_Amount25')\n",
        "# sd_Taxes.show(5)\n",
        "\n",
        "sd_tax_table = spark.sql(\"\"\"SELECT \n",
        "                          Order_Id, \n",
        "                          Order_Date,\n",
        "                          Order_Amount,\n",
        "                          Tax as Total_Tax,\n",
        "                          COALESCE(COALESCE(Consumption_Tax_VAT_12_5_,Con_Tax_VAT_12_5_),0)  as Consumption_Tax ,\n",
        "                          COALESCE(VAT_7_5_,0) as VAT,\n",
        "                          COALESCE(COALESCE(Net_Amount19, Net_Amount21),Net_Amount25) as Net_Amount_After_Tax\n",
        "                          FROM sd_Total_Orders\n",
        "                          WHERE Tax > 0\"\"\")\n",
        "\n",
        "sd_tax_table.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xMW2j6MXkQf",
        "outputId": "8a98421f-0f1f-48c8-fa89-c629e1df1967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+----------------+----------------+-----------------+--------------------+\n",
            "|Order_Id|Order_Date|Order_Amount|       Total_Tax| Consumption_Tax|              VAT|Net_Amount_After_Tax|\n",
            "+--------+----------+------------+----------------+----------------+-----------------+--------------------+\n",
            "|  281511|2022-01-01|      4500.0|           500.0|           500.0|              0.0|              4000.0|\n",
            "|  281525|2022-01-01|     10700.0|746.511627906976|             0.0|746.5116279069761|   9953.488372093025|\n",
            "|  281530|2022-01-01|      2300.0|255.555555555556|255.555555555556|              0.0|   2044.444444444444|\n",
            "|  281531|2022-01-01|      1100.0|122.222222222222|122.222222222222|              0.0|    977.777777777778|\n",
            "|  281532|2022-01-01|       200.0|22.2222222222222|22.2222222222222|              0.0|   177.7777777777778|\n",
            "+--------+----------+------------+----------------+----------------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the Table which would be used in creating sd_final_total_orders  from the sql view created above\n",
        "\n",
        "\n",
        "sd_final_total_orders_raw = spark.sql(\"\"\"SELECT \n",
        "                                        Order_Id, Order_Date, Order_Time, trim(Location) as Store, \n",
        "                                        Order_Taken_By, Order_Amount,trim(Payments) as Payments, trim(Notes) as Notes\n",
        "                                        FROM sd_Total_Orders\n",
        "                                      \"\"\")\n",
        "\n",
        "# sd_final_total_orders_raw.show(5)"
      ],
      "metadata": {
        "id": "e5bazA59hJ6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Extract The Payment_Channel from the  Payments column, \n",
        "# removing  white spaces and hyphen after the text then removing () afterwards using the substring function\n",
        "# replacing all special characters in the Notes column with '' to avoid database application errors.\n",
        "from pyspark.sql.functions import trim,initcap\n",
        "\n",
        "sd_final_total_orders_raw_2 = sd_final_total_orders_raw.withColumn(\"Payment_Channel\",substring_index(substring_index(col('Payments'),' -',1), '()',1)\n",
        "                                                      ).withColumn(\"Notes\", initcap(trim(regexp_replace(col(\"Notes\"), r'\\W+',\" \"))))\n",
        "\n",
        "# sd_final_total_orders_raw_2.filter(\"Notes is NOT NULL\").show(10, truncate= False)\n",
        "\n",
        "# preview\n",
        "# sd_final_total_orders_raw_2.select(col('Payment_Channel')).distinct().show()\n",
        "\n",
        "# code below didn't work due to the factors that needed to be replaced \n",
        "  # withColumn(\n",
        "              # \"Payment_Channel_2\",substring_index(col('Payments'),' -',1)).select(col('Payment_Channel_2')).distinct().show()\n",
        "              # withColumn(\"Payment_Channel\",regexp_extract(col('Payments'), '[a-zA-Z]+',0)\n",
        "\n"
      ],
      "metadata": {
        "id": "KUDhm9AVuBZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Final Addition of Columns**"
      ],
      "metadata": {
        "id": "u-__FvSN9Z5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sd_final_total_orders_raw_3 = sd_final_total_orders_raw_2.withColumn('Payment_Mode', F.when((col(\"Payment_Channel\")) == \"Cash\",\"Cash\").otherwise(\"Non Cash\")\n",
        "                                            ).withColumn('Order_Products_Price', lit(None).cast(FloatType())\n",
        "                                            ).withColumn('Order_Extra_Items_Price', lit(None).cast(FloatType())\n",
        "                                            ).withColumn('Order_Type', lit(None).cast(StringType())\n",
        "                                            ).withColumn('Order_State', lit(None).cast(StringType())\n",
        "                                            ).withColumn('Order_Source', lit(None).cast(StringType())\n",
        "                                            ).select('Order_Id', 'Order_Date', 'Order_Time','Store',\n",
        "                                                     'Order_Taken_By','Order_Products_Price',\n",
        "                                                      'Order_Extra_Items_Price',\n",
        "                                                     'Order_Amount','Payment_Channel','Payment_Mode',\n",
        "                                                     'Order_Type', 'Order_State','Order_Source','Notes'\n",
        "                                                     )\n",
        "                                            # .sort(col(\"Order_Date\").asc(),col(\"Order_Time\").asc())\n",
        "\n",
        "# sd_final_total_orders_raw_3.show(5)"
      ],
      "metadata": {
        "id": "eVOBqlet9YpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing Names within the Store Column**\n"
      ],
      "metadata": {
        "id": "KRfa-YOYENqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviewing the distinct store names\n",
        "\n",
        "raw_store_names = [store[0] for store in sd_final_total_orders_raw_3.select('Store').distinct().collect()]\n",
        "\n",
        "print(raw_store_names)\n",
        "\n",
        "# Next ,I'll define the raw list of actual_Store names\n",
        "actual_store_names = ['xxxxxx','xxxxxxxxxx','xxxxxxxxxxx']\n",
        "# sd_final_total_orders = sd_final_total_orders.replace([])\n",
        "\n",
        "sd_final_total_orders_raw_4 = sd_final_total_orders_raw_3.replace( raw_store_names, actual_store_names)\n",
        "\n",
        "# sd_final_total_orders_raw_4.show(5)\n",
        "sd_final_total_orders_raw_4.printSchema()\n",
        "\n",
        "sd_final_total_orders_raw_4.count()\n",
        "\n"
      ],
      "metadata": {
        "id": "FcKJo4d6EchE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get product Count for each order from the products table\n",
        "\n",
        "sd_final_total_orders_raw_4.createOrReplaceTempView('finalTotalOrders')\n",
        "\n",
        "\n",
        "sd_final_total_orders = spark.sql(\"\"\"\n",
        "                                  \n",
        "                                  WITH prodcountTable AS(\n",
        "                                    SELECT Order_Id, Order_Date, COUNT(Product) as Product_Count\n",
        "                                    FROM finalProductOrders\n",
        "                                    WHERE Product_Has_Price = 1\n",
        "                                    GROUP BY 1,2\n",
        "                                  )\n",
        "                                  SELECT CAST(T.Order_Id as String) as Order_Id, T.Order_Date, T.Order_Time, T.Store, T.Order_Taken_By,\n",
        "                                            T.Order_Products_Price,CAST(COALESCE(P.Product_Count,0) AS INT) as Product_Count, \n",
        "                                            T.Order_Extra_Items_Price,T.Order_Amount,T.Payment_Channel,\n",
        "                                            T.Payment_Mode,T.Order_Type, T.Order_State, T.Order_Source,T.Notes\n",
        "                                     FROM finalTotalOrders as T\n",
        "                                     LEFT JOIN prodcountTable as P\n",
        "                                     ON T.Order_Id = P.Order_Id\n",
        "                                     AND T.Order_Date = P.Order_Date\n",
        "                                     ORDER BY 2, 3\n",
        "                                      \"\"\")\n",
        "\n",
        "# sd_final_total_orders.show(5)\n",
        "sd_final_total_orders.printSchema()\n",
        "sd_final_total_orders.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8npOc2fvU-yR",
        "outputId": "c3d66c95-dc4c-4e64-842d-6ee899843ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Order_Id: string (nullable = true)\n",
            " |-- Order_Date: date (nullable = true)\n",
            " |-- Order_Time: string (nullable = true)\n",
            " |-- Store: string (nullable = true)\n",
            " |-- Order_Taken_By: string (nullable = true)\n",
            " |-- Order_Products_Price: float (nullable = true)\n",
            " |-- Product_Count: integer (nullable = false)\n",
            " |-- Order_Extra_Items_Price: float (nullable = true)\n",
            " |-- Order_Amount: float (nullable = true)\n",
            " |-- Payment_Channel: string (nullable = true)\n",
            " |-- Payment_Mode: string (nullable = false)\n",
            " |-- Order_Type: string (nullable = true)\n",
            " |-- Order_State: string (nullable = true)\n",
            " |-- Order_Source: string (nullable = true)\n",
            " |-- Notes: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72329"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# last preview to be sure that no order_amount is null\n",
        "null_order_amounts = spark.sql('Select * from finalTotalOrders WHERE Order_Amount IS NULL')\n",
        "null_order_amounts.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmUyTz4m1Uf2",
        "outputId": "ccc4e7d3-4c2b-46f2-be44-d77041379730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Export to csv File**\n",
        " - This format will be used in  Postgres DB"
      ],
      "metadata": {
        "id": "gFs7QljYOa5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sd_final_total_orders.coalesce(1).write.option(\"header\",True\n",
        "                                              #  ).csv(\"/content/drive/MyDrive/SD Raw Data/Transformed_Output/sd_total_orders_Jan_Feb_Apr_3\"\n",
        "                                                      #  )"
      ],
      "metadata": {
        "id": "-nQoVgQjO10I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}